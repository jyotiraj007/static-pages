<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Learning Diffusion Models: An Interactive Infographic</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700;900&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #F0F4F8;
        }
        .chart-container {
            position: relative;
            width: 100%;
            max-width: 600px;
            margin-left: auto;
            margin-right: auto;
            height: 320px;
            max-height: 400px;
        }
        @media (min-width: 768px) {
            .chart-container {
                height: 350px;
            }
        }
        .flow-arrow {
            position: relative;
            width: 100%;
            height: 40px;
            display: flex;
            align-items: center;
            justify-content: center;
        }
        .flow-arrow::after {
            content: '‚ñº';
            font-size: 2rem;
            color: #118AB2;
        }
        .flow-box {
            border: 2px solid #073B4C;
            background-color: white;
            color: #073B4C;
        }
        .timeline-line {
            position: absolute;
            left: 50%;
            top: 0;
            bottom: 0;
            width: 4px;
            background-color: #118AB2;
            transform: translateX(-50%);
        }
        .timeline-item {
            position: relative;
        }
        .timeline-dot {
            position: absolute;
            left: 50%;
            top: 50%;
            width: 24px;
            height: 24px;
            background-color: #FFD166;
            border: 4px solid #118AB2;
            border-radius: 50%;
            transform: translate(-50%, -50%);
            z-index: 10;
        }
        .timeline-content {
            width: calc(50% - 40px);
        }
        .timeline-item:nth-child(odd) .timeline-content {
            margin-left: auto;
            text-align: left;
        }
        .timeline-item:nth-child(even) .timeline-content {
            margin-right: auto;
            text-align: right;
        }
        .timeline-item:nth-child(even) .timeline-dot {
            left: 50%;
        }
    </style>
</head>
<body class="text-[#073B4C]">

    <header class="bg-[#073B4C] text-white text-center py-12 px-4">
        <h1 class="text-4xl md:text-6xl font-black tracking-tight">Diffusion Models</h1>
        <p class="text-xl md:text-2xl mt-4 font-light text-[#a7d8e8]">A Practical, Hands-On Guide to Generative AI</p>
    </header>

    <main class="container mx-auto p-4 md:p-8">

        <section id="introduction" class="text-center max-w-4xl mx-auto mb-16">
            <h2 class="text-3xl font-bold text-[#118AB2] mb-4">What Are Diffusion Models?</h2>
            <p class="text-lg leading-relaxed">
                Diffusion models are a powerful class of generative AI that learn to create new data by reversing a gradual noising process. Imagine taking a clear image, slowly adding static until it's pure noise, and then training a model to meticulously undo that process. This "denoising" approach allows them to generate stunningly realistic and diverse images, audio, and more from a simple random input.
            </p>
        </section>

        <section id="core-mechanics" class="mb-16">
            <h2 class="text-3xl font-bold text-center text-[#118AB2] mb-8">The Core Mechanic: A Two-Way Street</h2>
            <div class="grid grid-cols-1 md:grid-cols-2 gap-8 items-center">
                <div class="bg-white rounded-lg shadow-lg p-6">
                    <h3 class="text-2xl font-bold mb-4 text-[#FF6B6B]">1. Forward Process (Noising)</h3>
                    <p class="mb-6">We start with a clean data sample (like an image) and systematically add a small amount of Gaussian noise over many steps. This is a fixed, non-learned process that gradually transforms the data into pure, unstructured noise.</p>
                    <div class="flex flex-col items-center space-y-4">
                        <div class="flow-box p-4 rounded-lg w-full text-center">üñºÔ∏è Clean Image ($x_0$)</div>
                        <div class="flow-arrow"></div>
                        <div class="flow-box p-4 rounded-lg w-full text-center">üñºÔ∏è + üîä Slightly Noisy ($x_t$)</div>
                        <div class="flow-arrow"></div>
                        <div class="flow-box p-4 rounded-lg w-full text-center">üîä Pure Noise ($x_T$)</div>
                    </div>
                </div>
                <div class="bg-white rounded-lg shadow-lg p-6">
                    <h3 class="text-2xl font-bold mb-4 text-[#06D6A0]">2. Reverse Process (Denoising)</h3>
                    <p class="mb-6">This is where the magic happens. A neural network (typically a U-Net) is trained to predict the noise that was added at each step. By subtracting this predicted noise iteratively, it can reconstruct a clean image from a random noise input.</p>
                    <div class="flex flex-col items-center space-y-4">
                        <div class="flow-box p-4 rounded-lg w-full text-center">üîä Pure Noise ($x_T$)</div>
                        <div class="flow-arrow"></div>
                        <div class="flow-box p-4 rounded-lg w-full text-center">üñºÔ∏è - üîä Less Noisy ($x_{t-1}$)</div>
                        <div class="flow-arrow"></div>
                        <div class="flow-box p-4 rounded-lg w-full text-center">üñºÔ∏è Generated Image ($x_0$)</div>
                    </div>
                </div>
            </div>
        </section>

        <section id="unet-architecture" class="mb-16 bg-white rounded-lg shadow-lg p-6 md:p-8">
            <h2 class="text-3xl font-bold text-center text-[#118AB2] mb-2">The Engine: U-Net Architecture</h2>
            <p class="text-center text-lg max-w-3xl mx-auto mb-8">
                The U-Net is the workhorse behind most diffusion models. Its unique "U" shape, with an encoder, a decoder, and "skip connections," makes it perfect for denoising. It can understand the overall image context while preserving fine-grained details.
            </p>
            <div class="w-full overflow-x-auto">
                <div class="flex items-center justify-center p-4 min-w-[600px]">
                    <div class="flex flex-col space-y-2 items-center">
                        <div class="p-3 bg-[#FF6B6B] text-white rounded-lg w-48 text-center">Input</div>
                        <div class="h-8 w-1 bg-[#FF6B6B]"></div>
                        <div class="p-3 bg-[#FF6B6B] text-white rounded-lg w-40 text-center">Conv + Pool</div>
                        <div class="h-8 w-1 bg-[#FF6B6B]"></div>
                        <div class="p-3 bg-[#FF6B6B] text-white rounded-lg w-32 text-center">Conv + Pool</div>
                    </div>
                    <div class="flex flex-col mx-4">
                        <div class="w-24 h-1 bg-[#118AB2] self-start" style="margin-top: 100px;"></div>
                        <div class="p-3 bg-[#118AB2] text-white rounded-lg w-24 text-center my-12">Bottleneck</div>
                        <div class="w-24 h-1 bg-[#118AB2] self-start" style="margin-bottom: 100px;"></div>
                    </div>
                    <div class="flex flex-col space-y-2 items-center">
                         <div class="p-3 bg-[#06D6A0] text-white rounded-lg w-32 text-center">Up-Conv</div>
                        <div class="h-8 w-1 bg-[#06D6A0]"></div>
                        <div class="p-3 bg-[#06D6A0] text-white rounded-lg w-40 text-center">Up-Conv</div>
                        <div class="h-8 w-1 bg-[#06D6A0]"></div>
                        <div class="p-3 bg-[#06D6A0] text-white rounded-lg w-48 text-center">Output</div>
                    </div>
                </div>
                <p class="text-center text-[#118AB2] font-semibold mt-2">Dotted lines represent skip connections, passing detail from the encoder to the decoder.</p>
            </div>
        </section>
        
        <section id="sampling" class="mb-16">
            <h2 class="text-3xl font-bold text-center text-[#118AB2] mb-8">Generating Images: Sampling Strategies</h2>
            <div class="grid grid-cols-1 md:grid-cols-2 gap-8">
                <div class="bg-white rounded-lg shadow-lg p-6">
                    <h3 class="text-2xl font-bold mb-4 text-[#FFD166]">DDPM vs. DDIM Sampling</h3>
                    <p class="mb-6">The sampler determines how the model denoises the initial random input. Different samplers offer a trade-off between speed, quality, and diversity.</p>
                    <div class="chart-container">
                        <canvas id="samplerChart"></canvas>
                    </div>
                    <p class="text-center mt-4">DDIM is significantly faster, requiring far fewer steps to produce a high-quality image, making it ideal for rapid generation.</p>
                </div>
                <div class="bg-white rounded-lg shadow-lg p-6">
                    <h3 class="text-2xl font-bold mb-4 text-[#FFD166]">Diversity vs. Reproducibility</h3>
                    <ul class="space-y-4 text-lg">
                        <li class="flex items-start">
                            <span class="text-2xl mr-3">üé≤</span>
                            <div><strong class="text-[#118AB2]">DDPM (Stochastic):</strong> Introduces randomness at each step. This means you get a unique, diverse output every time, which is great for creative exploration.</div>
                        </li>
                        <li class="flex items-start">
                            <span class="text-2xl mr-3">üéØ</span>
                            <div><strong class="text-[#06D6A0]">DDIM (Deterministic):</strong> Follows a fixed path. Given the same starting noise, it will always produce the exact same image, ensuring reproducibility.</div>
                        </li>
                    </ul>
                </div>
            </div>
        </section>

        <section id="conditional-generation" class="mb-16 bg-white rounded-lg shadow-lg p-6 md:p-8">
            <h2 class="text-3xl font-bold text-center text-[#118AB2] mb-2">Taking Control: Conditional Generation</h2>
            <p class="text-center text-lg max-w-3xl mx-auto mb-8">
                We can guide the generation process using external information like text prompts. This is the foundation of powerful text-to-image models like Stable Diffusion.
            </p>
            <div class="grid grid-cols-1 md:grid-cols-3 gap-6 text-center">
                <div class="border-2 border-dashed border-[#FF6B6B] p-4 rounded-lg">
                    <h3 class="text-xl font-bold mb-2">1. Text Prompt</h3>
                    <p class="text-2xl font-mono p-2 bg-gray-100 rounded">"An astronaut riding a horse on Mars"</p>
                </div>
                <div class="flex items-center justify-center text-4xl text-[#118AB2] font-bold">‚Üí</div>
                <div class="border-2 border-dashed border-[#06D6A0] p-4 rounded-lg">
                    <h3 class="text-xl font-bold mb-2">2. Guided Generation</h3>
                    <p>The text is converted to an embedding that steers the U-Net's denoising process at each step, ensuring the output matches the prompt.</p>
                </div>
            </div>
            <div class="mt-8">
                <h3 class="text-2xl font-bold text-center mb-4 text-[#FFD166]">Classifier-Free Guidance</h3>
                <p class="text-center max-w-2xl mx-auto">This clever technique improves how well the output matches the prompt. By sometimes training the model *without* the text prompt, it learns the difference between conditional and unconditional generation. During sampling, we can amplify this difference to push the output even closer to the prompt's description.</p>
            </div>
        </section>

        <section id="voice-cloning" class="mb-16">
            <h2 class="text-3xl font-bold text-center text-[#118AB2] mb-8">Advanced Application: Voice Cloning</h2>
            <div class="grid grid-cols-1 md:grid-cols-2 gap-8">
                <div class="bg-white rounded-lg shadow-lg p-6">
                    <h3 class="text-2xl font-bold mb-4 text-[#06D6A0]">From Waveforms to Spectrograms</h3>
                    <p class="mb-6">Directly generating raw audio (waveforms) is hard. Instead, models often generate a **Mel Spectrogram**, which is a visual representation of sound. This 2D format is perfect for a U-Net to process. A separate component called a **Vocoder** then converts the final spectrogram back into audible sound.</p>
                    <div class="chart-container">
                        <canvas id="audioProcessChart"></canvas>
                    </div>
                </div>
                <div class="bg-white rounded-lg shadow-lg p-6">
                    <h3 class="text-2xl font-bold mb-4 text-[#06D6A0]">High-Quality TTS Systems</h3>
                    <p class="mb-6">Models like **Tortoise TTS** use a complex pipeline of multiple neural networks to achieve high-fidelity voice cloning from just a few seconds of reference audio.</p>
                    <div class="space-y-3">
                        <div class="flow-box p-3 rounded-md text-center">Text Input</div>
                        <div class="text-center text-xl">‚Üì</div>
                        <div class="flow-box p-3 rounded-md text-center">Acoustic Model (generates spectrogram)</div>
                        <div class="text-center text-xl">‚Üì</div>
                        <div class="flow-box p-3 rounded-md text-center">Diffusion Decoder (refines spectrogram)</div>
                        <div class="text-center text-xl">‚Üì</div>
                        <div class="flow-box p-3 rounded-md text-center">Vocoder (converts to audio)</div>
                        <div class="text-center text-xl">‚Üì</div>
                        <div class="flow-box p-3 rounded-md text-center">üîä Cloned Voice Output</div>
                    </div>
                </div>
            </div>
        </section>

        <section id="home-lab" class="mb-16">
            <h2 class="text-3xl font-bold text-center text-[#118AB2] mb-8">Build Your Lab: Hardware for Diffusion</h2>
            <div class="grid grid-cols-1 lg:grid-cols-2 gap-8">
                <div class="bg-white rounded-lg shadow-lg p-6">
                    <h3 class="text-2xl font-bold mb-4 text-[#FF6B6B]">GPU is King: VRAM Matters Most</h3>
                    <p class="mb-6">The GPU is the most critical component. VRAM (GPU memory) is the biggest bottleneck. More VRAM allows you to work with larger, higher-resolution models. Here's a look at consumer-grade NVIDIA options.</p>
                    <div class="chart-container h-96">
                        <canvas id="gpuChart"></canvas>
                    </div>
                </div>

                <div class="bg-white rounded-lg shadow-lg p-6">
                    <h3 class="text-2xl font-bold mb-4 text-[#FF6B6B]">An Incremental Upgrade Path</h3>
                    <p class="mb-6">You don't need to buy everything at once. Start with a solid foundation and upgrade components as your skills and needs grow.</p>
                    <div class="relative py-4">
                        <div class="timeline-line"></div>
                        <div class="timeline-item flex items-center mb-12">
                            <div class="timeline-content bg-gray-100 p-4 rounded-lg shadow">
                                <h4 class="font-bold">Initial Build (Appetizer)</h4>
                                <p>RTX 4070 Ti (12GB), 32GB RAM, 8-core CPU. Great for learning and entry-level projects.</p>
                            </div>
                            <div class="timeline-dot"></div>
                        </div>
                        <div class="timeline-item flex items-center mb-12">
                            <div class="timeline-dot"></div>
                            <div class="timeline-content bg-gray-100 p-4 rounded-lg shadow">
                                <h4 class="font-bold">Year 1 Upgrade (Main Course)</h4>
                                <p>Upgrade to RTX 4090 (24GB) and 64GB RAM. Tackle larger models and faster inference.</p>
                            </div>
                        </div>
                        <div class="timeline-item flex items-center">
                            <div class="timeline-content bg-gray-100 p-4 rounded-lg shadow">
                                <h4 class="font-bold">Year 2 Upgrade (Dessert)</h4>
                                <p>Add a second GPU, upgrade to a 12/16-core CPU, and 128GB RAM for professional-grade workloads.</p>
                            </div>
                            <div class="timeline-dot"></div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

    </main>

    <footer class="bg-[#073B4C] text-white text-center py-8 px-4 mt-8">
        <p class="text-lg">Now you have the roadmap. Start experimenting, building, and creating!</p>
        <p class="text-sm mt-2 opacity-70">Infographic generated based on "A Comprehensive Hands-On Guide to Diffusion Models".</p>
    </footer>

    <script>
        const tooltipTitleCallback = (tooltipItems) => {
            const item = tooltipItems[0];
            let label = item.chart.data.labels[item.dataIndex];
            if (Array.isArray(label)) {
                return label.join(' ');
            }
            return label;
        };

        const wrapLabel = (label, maxWidth) => {
            const words = label.split(' ');
            const lines = [];
            let currentLine = '';
            for (const word of words) {
                if ((currentLine + word).length > maxWidth && currentLine.length > 0) {
                    lines.push(currentLine);
                    currentLine = word;
                } else {
                    currentLine += (currentLine.length > 0 ? ' ' : '') + word;
                }
            }
            lines.push(currentLine);
            return lines;
        };

        const palette = {
            red: '#FF6B6B',
            yellow: '#FFD166',
            green: '#06D6A0',
            blue: '#118AB2',
            navy: '#073B4C',
            lightBlue: '#a7d8e8'
        };

        const samplerCtx = document.getElementById('samplerChart').getContext('2d');
        new Chart(samplerCtx, {
            type: 'bar',
            data: {
                labels: ['DDPM (Stochastic)', 'DDIM (Deterministic)'],
                datasets: [{
                    label: 'Typical Inference Steps',
                    data: [1000, 50],
                    backgroundColor: [palette.blue, palette.green],
                    borderColor: [palette.navy, palette.navy],
                    borderWidth: 2
                }]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                indexAxis: 'y',
                scales: {
                    x: {
                        beginAtZero: true,
                        title: {
                            display: true,
                            text: 'Number of Steps (Lower is Faster)',
                            color: palette.navy,
                            font: { size: 14 }
                        },
                        ticks: { color: palette.navy }
                    },
                    y: {
                        ticks: { color: palette.navy, font: { size: 14 } }
                    }
                },
                plugins: {
                    legend: { display: false },
                    title: {
                        display: true,
                        text: 'Inference Speed: DDPM vs. DDIM',
                        font: { size: 18, weight: 'bold' },
                        color: palette.navy,
                        padding: { bottom: 20 }
                    },
                    tooltip: { callbacks: { title: tooltipTitleCallback } }
                }
            }
        });

        const audioProcessCtx = document.getElementById('audioProcessChart').getContext('2d');
        new Chart(audioProcessCtx, {
            type: 'doughnut',
            data: {
                labels: ['Acoustic Model (Text to Spectrogram)', 'Diffusion Model (Refines Spectrogram)', 'Vocoder (Spectrogram to Waveform)'],
                datasets: [{
                    data: [40, 40, 20],
                    backgroundColor: [palette.blue, palette.green, palette.yellow],
                    borderColor: '#ffffff',
                    borderWidth: 4
                }]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                plugins: {
                    legend: {
                        position: 'bottom',
                        labels: { color: palette.navy, font: { size: 12 } }
                    },
                    title: {
                        display: true,
                        text: 'Core Components of a TTS System',
                        font: { size: 18, weight: 'bold' },
                        color: palette.navy,
                        padding: { bottom: 20 }
                    },
                    tooltip: { callbacks: { title: tooltipTitleCallback } }
                }
            }
        });

        const gpuCtx = document.getElementById('gpuChart').getContext('2d');
        const gpuLabels = ['NVIDIA RTX 4070 Ti', 'NVIDIA RTX 4080', 'NVIDIA RTX 4090', 'NVIDIA RTX A6000 (Pro)'];
        new Chart(gpuCtx, {
            type: 'bar',
            data: {
                labels: gpuLabels.map(l => wrapLabel(l, 16)),
                datasets: [{
                    label: 'VRAM (GB)',
                    data: [12, 16, 24, 48],
                    backgroundColor: [palette.red, palette.yellow, palette.green, palette.blue],
                    borderColor: palette.navy,
                    borderWidth: 2
                }]
            },
            options: {
                responsive: true,
                maintainAspectRatio: false,
                scales: {
                    y: {
                        beginAtZero: true,
                        title: {
                            display: true,
                            text: 'VRAM in Gigabytes (GB)',
                            color: palette.navy,
                            font: { size: 14 }
                        },
                        ticks: { color: palette.navy }
                    },
                    x: {
                        ticks: { color: palette.navy, font: { size: 12 } }
                    }
                },
                plugins: {
                    legend: { display: false },
                    title: {
                        display: true,
                        text: 'GPU VRAM Comparison',
                        font: { size: 18, weight: 'bold' },
                        color: palette.navy,
                        padding: { bottom: 20 }
                    },
                    tooltip: { callbacks: { title: tooltipTitleCallback } }
                }
            }
        });
    </script>
</body>
</html>
